{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## Team ID: YEAH\n",
    "## Group Member: Jinzhu Yang(jy3024), Jiayun Ni(jn2722), Saier Gong(sg3772)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aim to reconstruct the Residual Attention Network(Attention-56 and Attention-92) and Naive Attention Model(NAL) and reproduce the error rates by training them with CIFAR-10, CIFAR-100 and ImageNet datasets, referencing the paper of Residual Attention Network for Image Classification.\n",
    "\n",
    "In this notebook, we demonstrate the name of each jupyter notebook that works on different models and the loss and accuracy of it. Some tables of summarized outputs would also be given to demonstrate the results more straightforward and clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveAttention56_CIFAR10_sg.ipynb\n",
    "This jupyter notebook works on the model of naive attention learning with the architecture of attention56 model using CIFAR-10 dataset. The test accuracy is 0.8205.\n",
    "![NaiveAttention56_CIFAR10](./images/NaiveAttention56_CIFAR10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveAttention92_CIFAR10_jn.ipynb\n",
    "This jupyter notebook works on the model of naive attention learning with the architecture of attention92 model using CIFAR-10 dataset. The test accuracy is 0.7443.\n",
    "![NaiveAttention92_CIFAR10](./images/NaiveAttention92_CIFAR10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention56_CIFAR10_sg.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention56 model using CIFAR-10 dataset. The test accuracy is 0.8341.\n",
    "![ResidualAttention56_CIFAR10](./images/ResidualAttention56_CIFAR10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention56_Channel_CIFAR10_sg.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention56 model with channel attention method using CIFAR-10 dataset. The test accuracy is 0.8084.\n",
    "![ResidualAttention56_Channel_CIFAR10](./images/ResidualAttention56_Channel_CIFAR10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ResidualAttention56_Spatial_CIFAR10_sg.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention56 model with spatial attention method using CIFAR-10 dataset. The test accuracy is 0.8084.\n",
    "![ResidualAttention56_Spatial_CIFAR10](./images/ResidualAttention56_Spatial_CIFAR10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention92_CIFAR10_jn.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention92 model using CIFAR-10 dataset. The test accuracy is 0.8379.\n",
    "![ResidualAttention92_CIFAR10](./images/ResidualAttention92_CIFAR10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention92_CIFAR10_Noise10_jn.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention92 model with noise level of 10% using CIFAR-10 dataset. The test accuracy is 0.7936.\n",
    "![ResidualAttention92_CIFAR10_Noise10](./images/ResidualAttention92_CIFAR10_Noise10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention92_CIFAR10_Noise30_jn.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention92 model with noise level of 30% using CIFAR-10 dataset. The test accuracy is 0.7501.\n",
    "![ResidualAttention92_CIFAR10_Noise30](./images/ResidualAttention92_CIFAR10_Noise30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention92_CIFAR10_Noise50_jn.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention92 model with noise level of 50% using CIFAR-10 dataset. The test accuracy is 0.6126.\n",
    "![ResidualAttention92_CIFAR10_Noise50](./images/ResidualAttention92_CIFAR10_Noise50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention92_CIFAR10_Noise70_jn.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention92 model with noise level of 70 using CIFAR-10 dataset. The test accuracy is 0.4517.\n",
    "![ResidualAttention92_CIFAR10_Noise70](./images/ResidualAttention92_CIFAR10_Noise70.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualAttention56_CIFAR100_sg.ipynb\n",
    "This jupyter notebook works on the model of residual attention learning with the architecture of attention56 model using CIFAR-100 dataset. The test accuracy is 0.5298.\n",
    "![NaiveAttention56_CIFAR100](./images/NaiveAttention56_CIFAR100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ResidualAttention92_CIFAR100_jn.ipynb\n",
    "This jupyter notebook works on the model of residual attention network with the architecture of attention92 model using CIFAR-10 dataset. The test accuracy is 0.5013.\n",
    "![ResidualAttention92_CIFAR100](./images/ResidualAttention92_CIFAR100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_ImageNet.ipynb\n",
    "This jupyter notebook works on the model of residual attention learning with the architecture of attention56 model using ImageNet dataset. The top-1 error rate is 0.5875 and the top-5 error rate is 0.019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Tables below show the same result as above but in the format of tables that would demonstrate the results more clearly and informatively with comparisons among models built.\n",
    "\n",
    "This table show the error rate and training time of the Attention Residual Learning model and Naive Residual Learning model built using attention56 and attention92 and trained by CIFAR-10 dataset.\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\begin{array}{c|cc}\n",
    "\\hline \n",
    "    \\text{Network} & \\text{ARL(Top-1 err.%)(Training Time)}  & \\text{NAL(Top-1 err. %)(Training Time)} \\\\\n",
    "\\hline\n",
    "    \\text{Attention-56}  & \\text{16.59(2h25min8s)} & \\text{17.95(2h59min58s)} \\\\\n",
    "    \\text{Attention-92}  & \\text{16.21(2h40min24s)} & \\text{25.57(2h26min3s)} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This table shows the error rate and training time of the Residual Attention Network built using different types of attention modules and trained by CIFAR-10 dataset.\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\begin{array}{c|cc}\n",
    "\\hline \n",
    "    \\text{Activation Function} & \\text{Attention Type}  & \\text{Top-1 err. (%)} &\\text{Training Time} \\\\\n",
    "\\hline\n",
    "    \\text{$f_1(x)$}  & \\text{Mixed Attention} & \\text{16.59} &\\text{2h25min8s} \\\\\n",
    "    \\text{$f_2(x)$}  & \\text{Channel Attention} & \\text{19.16} &\\text{1h56min53s} \\\\\n",
    "    \\text{$f_3(x)$}  & \\text{Spatial Attention} & \\text{15.36}  &\\text{4h43min24s}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This table demonstrates the error rate and training time of the Residual Attention Network built using attention92 model with different level of noise and trained by CIFAR-10 dataset.\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\begin{array}{c|cc}\n",
    "\\hline \n",
    "    \\text{Noise Level} & \\text{Attention-92 err.(%)}  & \\text{Training Time} \\\\\n",
    "\\hline\n",
    "    \\text{10%}  & \\text{20.64} & \\text{2h40min55s} \\\\\n",
    "    \\text{30%}  & \\text{24.99} & \\text{1h50min55s} \\\\\n",
    "    \\text{50%}  & \\text{38.74} & \\text{1h20min20s} \\\\\n",
    "    \\text{70%}  & \\text{54.83} & \\text{1h51min42s} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This table demonstrates the error rate and training time of the Residual Attention Network built using attention56 and attention92 model and trained by CIFAR-10 and CIFAR-100 datasets.\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\begin{array}{c|cc}\n",
    "\\hline \n",
    "    \\text{Network} & \\text{CIFAR-10(Training Time)}  & \\text{CIFAR-100(Training Time)} \\\\\n",
    "\\hline\n",
    "    \\text{Attention-56}  & \\text{16.59(2h25min8s)} & \\text{47.20(3h18min)} \\\\\n",
    "    \\text{Attention-92}  & \\text{16.21(2h40min24s)} & \\text{49.87(1h57min50s)} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This table listed the training accuracy, validation accuracy, top-1 test error rate and top-5 test error rate of the Residual Attention Network built using attention56 and trained by ImageNet dataset.\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\begin{array}{c|cccc}\n",
    "\\hline \n",
    "    \\text{Network} & \\text{Training Accuracy}  & \\text{Validation Accuracy} & \\text{Test Top-1 Error} & \\text{Test Top-5 Error} \\\\\n",
    "\\hline\n",
    "    \\text{Attention-56}  & \\text{0.3614} & \\text{0.3605} & \\text{0.5875} & \\text{0.0019} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
